Activity A: Preparing to Train a Predictive Model for the Employee-Retention Problem
Assessing the features

Step 2

for f in df.columns:
    try:
        fig = plt.figure()
        num_bins = min((30, len(df[f].unique())))
        df[f].hist(bins=num_bins)
        plt.xlabel(f)
        plt.show()
    except TypeError:
        print()
        print(df[f].value_counts())
        print('-'*30)


**************************************************************************************

Subtopic B: Assessing Models with k-Fold Testing and Validation Curves
Exercise - Using k-fold cross validation and validation curves in Python with scikit-learn

Step 6

from sklearn.model_selection import StratifiedKFold

def cross_val_class_score(clf, X_train, y_train):
    kfold = StratifiedKFold(n_splits=10,
                            random_state=1)\
                .split(X_train, y_train)

    class_accuracy = []
    for k, (train, test) in enumerate(kfold):
        clf.fit(X_train[train], y_train[train])
        y_test = y_train[test]
        y_pred = clf.predict(X_train[test])
        cmat = confusion_matrix(y_test, y_pred)
        class_acc = cmat.diagonal()/cmat.sum(axis=1)
        class_accuracy.append(class_acc)
        print('fold: {:d} accuracy: {:s}'.format(k+1, str(class_acc)))
        
    return class_accuracy


**************************************************************************************

Subtopic B: Assessing Models with k-Fold Testing and Validation Curves
Exercise - Using k-fold cross validation and validation curves in Python with scikit-learn

Step 7

from sklearn.model_selection import cross_val_score

np.random.seed(1)
scores = cross_val_class_score(clf, X, y)

print('accuracy = {} +/- {}'\
        .format(scores.mean(axis=0), scores.std(axis=0)))
>> fold: 1 accuracy: [ 0.99125109  0.75139665]
>> fold: 2 accuracy: [ 0.98950131  0.67787115]
>> fold: 3 accuracy: [ 0.98950131  0.70868347]
>> fold: 4 accuracy: [ 0.98600175  0.70308123]
>> fold: 5 accuracy: [ 0.98512686  0.71708683]
>> fold: 6 accuracy: [ 0.9816273   0.75910364]
>> fold: 7 accuracy: [ 0.98862642  0.69747899]
>> fold: 8 accuracy: [ 0.9816273   0.72268908]
>> fold: 9 accuracy: [ 0.99036778  0.72829132]
>> fold: 10 accuracy: [ 0.98861646  0.70588235]
>> accuracy = [ 0.98722476  0.71715647] +/- [ 0.00330026  0.02326823]


******************************************************************************************

Subtopic C: Dimensionality Reduction Techniques
Exercise - Training a predictive model for the employee retention problem

Step 2

features = ['satisfaction_level', 'last_evaluation', 'number_project',
     'average_montly_hours', 'time_spend_company', 'work_accident',
     'promotion_last_5years', 'department_IT', 'department_RandD',
     'department_accounting', 'department_hr', 'department_management',
     'department_marketing', 'department_product_mng', 'department_sales',
     'department_support', 'department_technical', 'salary_high',
     'salary_low', 'salary_medium']

X = df[features].values
y = df.left.values


******************************************************************************************

Subtopic C: Dimensionality Reduction Techniques
Exercise - Training a predictive model for the employee retention problem

Step 8

from sklearn.decomposition import PCA

pca_features = \
      ['work_accident', 'salary_low', 'salary_high', 'salary_medium',
       'promotion_last_5years', 'department_RandD', 'department_hr',
       'department_technical', 'department_support',
       'department_management', 'department_sales',
       'department_accounting', 'department_IT', 'department_product_mng',
       'department_marketing']
X_reduce = df[pca_features]

pca = PCA(n_components=3)
X_pca = pca.fit_transform(X_reduce)



